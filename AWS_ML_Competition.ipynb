{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8JMVCGIxQci"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G9myaJDxsOu"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/insiya24/dataset/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfxTVah_PoAl",
        "outputId": "2dcc556e-a933-429b-f481-f13befca3041"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 6)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3Iyow5lPs7E",
        "outputId": "a87daef2-b29d-4771-e395-4f1ead60a96a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PRODUCT_ID             0\n",
              "TITLE                  1\n",
              "BULLET_POINTS      37188\n",
              "DESCRIPTION        51570\n",
              "PRODUCT_TYPE_ID        0\n",
              "PRODUCT_LENGTH         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bnRm8vhaOM3",
        "outputId": "48238dd5-d5a4-47d8-cf80-2d6982110d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 6)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.fillna(\"not mentioned\")\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFZGOGoF2dFH",
        "outputId": "2b03055b-006d-4dab-f988-4103a200d48f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "\n",
        "# Define a function to preprocess and tokenize the text\n",
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if not token in stop_words]\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    # Return the preprocessed and tokenized text as a single string\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "\n",
        "# Apply the preprocessing function to each column of text and store the union of tokens in a new column\n",
        "df['TOKENS'] = df['TITLE'].apply(preprocess_text) + ' ' + df['BULLET_POINTS'].apply(preprocess_text) + ' ' + df['DESCRIPTION'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drnyPM4nbUr5",
        "outputId": "7fe33b3a-dcda-4d86-d8bc-145a14a5dccb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        artzfolio tulip flower blackout curtain door w...\n",
              "1        mark spencer girl pyjama set t862561cnavy mix9...\n",
              "2        priknik horn red electric air horn compressor ...\n",
              "3        alishah woman cotton ankle length legging comb...\n",
              "4        united empire loyalist chronicle great migrati...\n",
              "                               ...                        \n",
              "99995    soffe girl big core tank top surf blue small g...\n",
              "99996    governance regulation international finance pr...\n",
              "99997    interfaith alternative embracing spiritual div...\n",
              "99998                               midnight rider nan nan\n",
              "99999    muellery recycling bag recycle box bin waterpr...\n",
              "Name: TOKENS, Length: 100000, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['TOKENS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15-qMzT02sLB"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Perform TF-IDF encoding on the tokens column\n",
        "tfidf = TfidfVectorizer(max_features=250)\n",
        "x = tfidf.fit_transform(df['TOKENS'])\n",
        "\n",
        "\n",
        "y = df['PRODUCT_LENGTH']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXAyBEWd2zxx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.225)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWvE52mW20Ui"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=1, # minimium amount of change to count as an improvement\n",
        "    patience=5, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(2048, activation='relu', input_shape=[250]),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(2048, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(2048, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_absolute_percentage_error', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBBYe29Z3Ush",
        "outputId": "704ad2ff-f767-4962-ab96-5fde3563ffa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "303/303 [==============================] - 17s 16ms/step - loss: 99.1903 - val_loss: 98.4002\n",
            "Epoch 2/10\n",
            "303/303 [==============================] - 4s 15ms/step - loss: 96.3036 - val_loss: 92.8771\n",
            "Epoch 3/10\n",
            "303/303 [==============================] - 6s 19ms/step - loss: 92.2428 - val_loss: 89.5357\n",
            "Epoch 4/10\n",
            "303/303 [==============================] - 5s 15ms/step - loss: 86.3952 - val_loss: 101.7260\n",
            "Epoch 5/10\n",
            "303/303 [==============================] - 5s 16ms/step - loss: 81.2393 - val_loss: 89.9279\n",
            "Epoch 6/10\n",
            "303/303 [==============================] - 4s 15ms/step - loss: 77.0770 - val_loss: 90.4129\n",
            "Epoch 7/10\n",
            "303/303 [==============================] - 5s 16ms/step - loss: 71.7295 - val_loss: 88.2683\n",
            "Epoch 8/10\n",
            "303/303 [==============================] - 5s 16ms/step - loss: 69.8441 - val_loss: 91.1877\n",
            "Epoch 9/10\n",
            "303/303 [==============================] - 5s 15ms/step - loss: 68.9165 - val_loss: 102.2495\n",
            "Epoch 10/10\n",
            "303/303 [==============================] - 5s 16ms/step - loss: 67.2629 - val_loss: 89.1894\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fed580fdca0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(x_train.toarray(), y_train, \n",
        "          epochs=10, \n",
        "          batch_size=256,\n",
        "          validation_data=(x_val.toarray(), y_val),\n",
        "          callbacks=[early_stopping],\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXhsZngO3cKo",
        "outputId": "56f7f864-6c57-449b-fdcf-bcd06b8d2be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "704/704 [==============================] - 2s 2ms/step\n",
            "Mean Absolute Percentage Error: 0.8918938405594382\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(x_val.toarray())\n",
        "\n",
        "# Calculate the mean absolute percentage error on the validation set\n",
        "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
        "print(\"Mean Absolute Percentage Error:\", mape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPalZR_A3oSI",
        "outputId": "2282c7e3-37fe-43d3-ea8a-67d4edd9705e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[708.4234 ],\n",
              "       [447.21732],\n",
              "       [446.8015 ],\n",
              "       ...,\n",
              "       [576.57056],\n",
              "       [536.9218 ],\n",
              "       [542.58405]], dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk7RKGPa4Dn5",
        "outputId": "7fd6e84f-49d6-4a93-b2f4-0f4088b17b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3125/3125 [==============================] - 8s 3ms/step\n",
            "Mean Absolute Percentage Error: 1.1383301312039371\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv(\"https://raw.githubusercontent.com/insiya24/dataset/test.csv\")\n",
        "\n",
        "# Apply the preprocessing function to each column of text and store the union of tokens in a new column\n",
        "test_df['TOKENS'] = test_df['TITLE'].apply(preprocess_text) + ' ' + test_df['BULLET_POINTS'].apply(preprocess_text) + ' ' + test_df['DESCRIPTION'].apply(preprocess_text)\n",
        "\n",
        "# Perform TF-IDF encoding on the tokens column\n",
        "tfidf = TfidfVectorizer(max_features=250)\n",
        "x_test = tfidf.fit_transform(test_df['TOKENS'])\n",
        "\n",
        "y_test = test_df['PRODUCT_LENGTH']\n",
        "\n",
        "y_test_pred = model.predict(x_test.toarray())\n",
        "\n",
        "# Calculate the mean absolute percentage error on the test set\n",
        "mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
        "print(\"Mean Absolute Percentage Error:\", mape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6G27tjklI_w",
        "outputId": "bb50b700-c80d-4079-868e-fb87a633d177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted Lenght  [449.68082]\n"
          ]
        }
      ],
      "source": [
        "print('The predicted Lenght ', y_test_pred[26])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfkmAQF3mA2v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
